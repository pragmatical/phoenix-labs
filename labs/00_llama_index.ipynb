{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import phoenix as px\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    set_global_handler,\n",
    ")\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# The API key for your Azure OpenAI resource.\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# Setting up the deployment name\n",
    "gpt_model_name = os.getenv(\"AZURE_OPENAI_GPT_MODEL\")\n",
    "embedding_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "api_type = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n",
    "session = px.launch_app()\n",
    "\n",
    "\n",
    "# Once you have started a Phoenix server, you can start your LlamaIndex application and configure it to send traces to Phoenix. To do this, you will have to add configure Phoenix as the global handler\n",
    "\n",
    "set_global_handler(\"arize_phoenix\")\n",
    "\n",
    "llm=AzureOpenAI(\n",
    "            azure_endpoint=base_url,\n",
    "            api_key=api_key,\n",
    "            deployment_name=gpt_model_name,\n",
    "            model=gpt_model_name,\n",
    "            api_version=api_version,\n",
    ")\n",
    "# LlamaIndex application initialization may vary\n",
    "# depending on your application\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = AzureOpenAIEmbedding(\n",
    "    azure_endpoint=base_url,\n",
    "    api_key=api_key,\n",
    "    deployment_name=embedding_model_name,\n",
    "    model=embedding_model_name,\n",
    "    api_version=api_version,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data and create an index. Note you usually want to store your index in a persistent store like a database or the file system\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query your LlamaIndex application\n",
    "query_engine.query(\"What is the meaning of life?\")\n",
    "query_engine.query(\"Why did the cow jump over the moon?\")\n",
    "\n",
    "# View the traces in the Phoenix UI\n",
    "px.active_session().url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
