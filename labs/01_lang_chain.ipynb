{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Launch phoenix\n",
    "session = px.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# The API key for your Azure OpenAI resource.\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# Setting up the deployment name\n",
    "gpt_model_name = os.getenv(\"AZURE_OPENAI_GPT_MODEL\")\n",
    "embedding_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "api_type = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have started a Phoenix server, you can start your LangChain application with the OpenInferenceTracer as a callback. To do this, you will have to instrument your LangChain application with the tracer:\n",
    "\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "# By default, the traces will be exported to the locally running Phoenix server.\n",
    "LangChainInstrumentor().instrument()\n",
    "\n",
    "# Initialize your LangChain application\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=embedding_model_name,\n",
    "    api_version=api_version,\n",
    ")\n",
    "documents_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/langchain-pinecone/database.parquet\"\n",
    ")\n",
    "knn_retriever = KNNRetriever(\n",
    "    index=np.stack(documents_df[\"text_vector\"]),\n",
    "    texts=documents_df[\"text\"].tolist(),\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=gpt_model_name,\n",
    "    api_version=api_version,\n",
    "    )\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=knn_retriever,\n",
    ")\n",
    "\n",
    "# Instrument the execution of the runs with the tracer. By default the tracer uses an HTTPExporter\n",
    "query = \"What is euclidean distance?\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# By adding the tracer to the callbacks of LangChain, we've created a one-way data connection between your LLM application and Phoenix.\n",
    "\n",
    "# To view the traces in Phoenix, simply open the UI in your browser.\n",
    "session.url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
